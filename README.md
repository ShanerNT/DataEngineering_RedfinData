# DataEngineering_RedfinData

This project downloads, cleans, and aggregates Redfin real estate data using Airflow, S3, and Snowflake.  
The high-level architecture is:

1. **Airflow** (running on an Amazon EC2 instance, or any environment you choose) executes a DAG that:
   - Downloads the TSV data from Redfin’s public S3 bucket.  
   - Transforms and cleans the data.  
   - Pushes the final CSV data to your own S3 bucket.  
2. **Snowpipe** (in Snowflake) automatically ingests data from the S3 bucket into a Snowflake table, triggered by **SQS** notifications.
3. **Reporting Tool Of Choice** Connect a reporting tool like PowerBI or Tableau to the dataset within Snowflake

## Table of Contents
1. [Prerequisites](#prerequisites)  
2. [Installation & Environment Setup](#installation--environment-setup)  
3. [Configuration File](#configuration-file)  
4. [Project Structure](#project-structure)  
5. [Airflow Setup & Running the DAG](#airflow-setup--running-the-dag)
6. [Snowflake Setup](#snowflake-setup)  
7. [AWS S3 and SQS Setup for Snowpipe](#aws-s3-and-sqs-setup-for-snowpipe)  
8. [Testing & Verification](#testing--verification)  
9. [Troubleshooting](#troubleshooting)  

---

## 1. Prerequisites
- **Python 3.9+** (or compatible version)
- **Amazon AWS Account** with access to:
  - S3 Bucket creation
  - SQS
  - IAM roles/policies for S3/SQS
- **Snowflake** account with privileges to create databases, schemas, pipes, etc.
- (Optional) **Amazon EC2** instance if you wish to run Airflow and Python scripts in the cloud.  

> **Note:** You can run Airflow on your local machine or any other environment instead of EC2, but these instructions use EC2 as an example.

---

## 2. Installation & Environment Setup

### 2.1. Launch an Amazon EC2 Instance (optional)
1. Log in to AWS and go to the **EC2** dashboard.  
2. Launch a new instance with an AMI of your choice (e.g., Amazon Linux 2).  
3. Make sure you allow inbound traffic for:
   - SSH (port 22) – so you can connect.
   - Airflow Webserver (port 8080) – if you want to view Airflow in the browser externally.
4. SSH into the instance once it’s running.

### 2.2. Python Environment Setup
1. **Install Python 3** (if not already installed) on your machine/instance.  
2. Create a virtual environment (recommended) and activate it. For example:
   ```bash
   python3 -m venv venv
   source venv/bin/activate
   ```
   
### 2.3. Install Required Python Libraries
  ```bash
  pip install apache-airflow==2.10.3
  pip install boto3==1.35.57
  pip install pandas==2.2.3
  ```
---
## 3. Configuration Files
1. The code references a JSON config file `config.json` containing keys for AWS and personal settings. An example structure might look like this:
    ```json
    {
      "aws": {
        "S3_BUCKET_NAME": "your-s3-bucket-name"
      },
      "personal": {
        "EMAIL": "your_email@example.com"
      }
    }
    ```
> **Note:**  You can store your AWS credentials (Access Key, Secret Key) either in the environment variables, ~/.aws/credentials, or use IAM roles.
Update the path to the config file in redfin_analytics.py if you store it elsewhere.
---
## 4. Project Structure
  ```arduino
    ├── airflow/
    │   └── dags/
    │       └── redfin_analytics.py          <-- The DAG script
    ├── snowflake_redfin_script.sql          <-- Snowflake setup SQL
    ├── ReadMe.md
    └── config.json
  ```

1. `redfin_analytics.py` Contains the Airflow DAG and associated Python functions for extracting, transforming, and loading Redfin data to S3.
2. `snowflake_redfin_script.sql` Snowflake SQL script to create a database, schema, table, stage, file format, and Snowpipe.
3. `config.json` JSON file storing configuration details (bucket name, email, etc.).
4. `ReadMe.md` Documentation (this file).

---
## 5. Airflow Setup & Running the DAG

This project uses **Airflow Standalone**, which is a quick way to get Airflow up and running locally or on an EC2 instance.

1. **Start Airflow Standalone**:  
   Run the following command in your terminal:
   ```bash
   airflow standalone
   ```
2. Get Airflow Username/Password:
   - Once the Airflow standalone server starts, the terminal will display a username and password for logging into the Airflow web interface.
   - Example output in the terminal:
      ```vbnet
      Username: admin
      Password: <auto-generated-password>
      ```
3. Access the Airflow Web Interface:
   - Open a web browser and navigate to `http://localhost:8080` (or `http://<your-EC2-public-IP>:8080` if running on an EC2 instance).
   - Use the username and password displayed in the terminal to log in.
4. Copy/Place the DAG file `redfin_analytics.py` into the `~/airflow/dags/` folder.
5. Trigger the DAG:
   - Once logged into the Airflow web interface, you will see your DAG (e.g., redfin_analytics_dag) listed.
   - Enable the DAG by toggling the On/Off switch.
   - You can trigger it manually by clicking the Trigger DAG button (play icon).

---
## 6. Snowflake Setup

Use the provided **`snowflake_redfin_script.sql`** to create the database, schema, table, stage, file format, and Snowpipe. Below is a summary:

1. **Connect to Snowflake** (e.g., via Snowflake UI or SnowSQL).  
2. **Run** the SQL script `snowflake_redfin_script.sql`:
   - Be sure to update your s3 URL `url="https://s3.us-east-1.amazonaws.com/..."` and aws credentials `aws_key_id     = 'YOUR_AWS_KEY' aws_secret_key = 'YOUR_AWS_SECRET'`
   - Check to make sure the file naming pattern matches your s3 directory `PATTERN = '^redfin/redfin_data.*'`;
   - Execute the script
4. Copy the “notification channel” from the DESC PIPE output.
   ```sql
   DESC PIPE redfin_database_1.snowpipe_schema.redfin_snowpipe;
   ```
    - It will look something like an ARN for an SQS queue that Snowflake manages: `arn:aws:sqs:us-east-1:123456789012:snowpipe-xyz-abcd-queue`
    - This will be used in the next step while setting up S3
   
---
## 7. AWS S3 and SQS Setup for Snowpipe

To enable Snowpipe to automatically ingest data from S3, you can configure a simple **event notification** in your S3 bucket.

### Steps:

1. **Create or Use an S3 Bucket**:  
   - Ensure you have an S3 bucket where the cleaned Redfin data will be uploaded.  
   - For example, `your-s3-bucket-name`.

2. **Enable Event Notifications**:  
   - Go to the S3 console and select your bucket.
   - Navigate to the **Properties** tab and scroll to **Event Notifications**.
   - Click **Create event notification** and fill in the details:
     - **Event name**: e.g., `redfin-snowpipe-event`
     - **Prefix**: `redfin/redfin` (this will get all files that start with the text 'redfin' within the redfin folder).
     - **Suffix**: `.csv` (to target only CSV files).
     - **Event types**: `All object create events`.
     - **Destination**: Select **SQS Queue** as the destination.
     - **Specify SQS Queue**: Enter SQS Queue ARN
        - Enter SQS Queue retrieved from Snowflake in prior step, or by running below query on snowflake:
         ```sql
         DESC PIPE redfin_database_1.snowpipe_schema.redfin_snowpipe;
         ```

4. **Test the Setup**:  
   - Upload a sample CSV file to your S3 bucket under the specified prefix (e.g., `redfin/`).
   - Check Snowflake to ensure the data is loaded automatically into your table.

By setting up a simple event notification, S3 will notify Snowflake whenever a new file matching the criteria is added, allowing Snowpipe to ingest the data seamlessly.

---
## 8. Testing & Verification

1. **Trigger the Airflow DAG**  
   - Via the Airflow UI or CLI. The DAG will:
     - Download the Redfin dataset from the public URL.
     - Transform and clean the data.
     - Upload the processed CSV to `your-s3-bucket-name/redfin/`.
   - The last step (`tsk_load_to_s3`) moves the raw CSV from EC2 to S3.  

2. **Check S3**  
   - Ensure a new object with prefix `redfin/redfin` (e.g., `redfin/redfin_data_XXXXXXXX.csv`) shows up in your bucket.

3. **Check Snowflake**  
   - Log in to Snowflake and query:
     ```sql
     SELECT COUNT(*) FROM redfin_database_1.redfin_schema.redfin_table;
     ```
   - If Snowpipe loaded successfully, the count should increase after the new file is added.

4. **Monitor SQS**  
   - You can look at the SQS queue to see if messages are being published from S3.
---
## 9. Troubleshooting

- **Permission Errors**:  
  - Ensure the IAM role attached to your EC2 (or local environment’s AWS CLI credentials) can read/write to S3.  
  - Make sure S3 can publish to your SQS queue (check Access Policy).  

- **Snowflake Stage Credentials**:  
  - Double-check your `aws_key_id` and `aws_secret_key` in the Snowflake `CREATE STAGE` command, or confirm you’re using an external stage with an IAM role.

- **Airflow Import Issues**:  
  - If Airflow doesn’t recognize the DAG, ensure the file is in the correct `airflow/dags/` directory and that the Python environment is consistent.

- **No Data in Snowflake Table**:  
  - Ensure your Snowpipe `PATTERN` in the script matches the file name(s) you are uploading.  
  - Confirm that S3 events are firing and that the SQS queue is receiving messages.
---





    
